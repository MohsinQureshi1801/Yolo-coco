{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COCO 2017 \u2192 YOLO Dataset Prep + Training (Colab)\n",
        "\n",
        "This notebook is restructured for a **production-style workflow** while staying fully compatible with **Google Colab**.\n",
        "\n",
        "## What this version improves\n",
        "- Cleaner separation of setup, configuration, data engineering, and training.\n",
        "- Better validation and error messages for paths and annotation files.\n",
        "- Idempotent label generation (no duplicate annotations on rerun).\n",
        "- Safer symlink creation with existence checks and summary logging.\n",
        "- Explicit class mapping validation against category names.\n",
        "- **Budget-aware training controls** for free Colab/Kaggle sessions (time-boxed training + resume).\n"
        "- Explicit class mapping validation against category names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab dependency setup\n",
        "!pip install -q ultralytics kagglehub pyyaml tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import yaml\n",
        "from tqdm.auto import tqdm\n",
        "from ultralytics import YOLO\n",
        "import kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: mount Google Drive if you want to read/write model/data configs there.\n",
        "USE_GOOGLE_DRIVE = True\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Configuration (single source of truth)\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Model/checkpoint\n",
        "    model_weights: str = '/content/drive/MyDrive/YOLOv5/coco_finetune/weights/last.pt'\n",
        "    fallback_model: str = 'yolo11n.pt'  # fast fallback for constrained GPUs\n",
        "\n",
        "    # Dataset source\n",
        "    kaggle_dataset: str = 'awsaf49/coco-2017-dataset'\n",
        "\n",
        "    # Output locations\n",
        "    work_dir: Path = Path('/content/yolo_dataset')\n",
        "    data_yaml_dir: Path = Path('/content/drive/MyDrive/YOLOv5/datasets/coco_finetune')\n",
        "\n",
        "    # Training (budget-aware defaults for free tiers)\n",
        "    # Training\n",
        "    train_project: str = '/content/drive/MyDrive/YOLOv5'\n",
        "    train_name: str = 'coco_finetune'\n",
        "    epochs: int = 50\n",
        "    imgsz: int = 640\n",
        "    batch: int = 16\n",
        "    workers: int = 2\n",
        "    device: int = 0\n",
        "\n",
        "    # Key controls for free Colab/Kaggle sessions\n",
        "    time_hours: float = 0.8         # stop before runtime limit (~48 min)\n",
        "    fraction: float = 0.25          # train on 25% data per run (increase later)\n",
        "    freeze_backbone_layers: int = 10  # freeze early layers to speed fine-tuning\n",
        "\n",
        "    workers: int = 4\n",
        "    device: int = 0\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "# COCO category_id -> custom YOLO class_id\n",
        "CLASS_MAPPING: Dict[int, int] = {\n",
        "    1: 0,   # person\n",
        "    3: 1,   # car\n",
        "    8: 2,   # truck\n",
        "    6: 3,   # bus\n",
        "    2: 4,   # bicycle\n",
        "    4: 5,   # motorcycle\n",
        "    10: 6,  # traffic_light\n",
        "    11: 7,  # fire_hydrant\n",
        "    13: 8,  # stop_sign\n",
        "}\n",
        "\n",
        "CLASS_NAMES: List[str] = [\n",
        "    'person', 'car', 'truck', 'bus', 'bicycle',\n",
        "    'motorcycle', 'traffic_light', 'fire_hydrant', 'stop_sign'\n",
        "]\n",
        "\n",
        "assert len(CLASS_NAMES) == len(set(CLASS_MAPPING.values())), 'CLASS_NAMES count must match mapped class IDs.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "\n",
        "def ensure_dir(path: Path) -> None:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def validate_coco_json(data: dict) -> None:\n",
        "    required_keys = {'images', 'annotations', 'categories'}\n",
        "    missing = required_keys - set(data.keys())\n",
        "    if missing:\n",
        "        raise ValueError(f'Missing required COCO keys: {missing}')\n",
        "\n",
        "\n",
        "def prepare_dirs(base_dir: Path) -> Dict[str, Path]:\n",
        "    dirs = {\n",
        "        'train_images': base_dir / 'final/images/train',\n",
        "        'val_images': base_dir / 'final/images/val',\n",
        "        'train_labels': base_dir / 'final/labels/train',\n",
        "        'val_labels': base_dir / 'final/labels/val',\n",
        "    }\n",
        "    for p in dirs.values():\n",
        "        ensure_dir(p)\n",
        "    return dirs\n",
        "\n",
        "\n",
        "def reset_label_dir(label_dir: Path) -> None:\n",
        "    if label_dir.exists():\n",
        "        shutil.rmtree(label_dir)\n",
        "    label_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def convert_coco_json_to_yolo(\n",
        "    json_path: Path,\n",
        "    output_label_dir: Path,\n",
        "    category_mapping: Dict[int, int],\n",
        "    clear_existing: bool = True,\n",
        ") -> Tuple[int, int]:\n",
        "    # Convert a COCO annotation JSON into YOLO txt labels.\n",
        "    # Returns: (written_boxes, skipped_boxes)\n",
        "    if clear_existing:\n",
        "        reset_label_dir(output_label_dir)\n",
        "    else:\n",
        "        ensure_dir(output_label_dir)\n",
        "\n",
        "    if not json_path.exists():\n",
        "        raise FileNotFoundError(f'Annotation file not found: {json_path}')\n",
        "\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    validate_coco_json(data)\n",
        "    images = {img['id']: img for img in data['images']}\n",
        "\n",
        "    written_boxes = 0\n",
        "    skipped_boxes = 0\n",
        "\n",
        "    for ann in tqdm(data['annotations'], desc=f'Converting {json_path.name}'):\n",
        "        cat_id = ann.get('category_id')\n",
        "        if cat_id not in category_mapping:\n",
        "            skipped_boxes += 1\n",
        "            continue\n",
        "\n",
        "        img_id = ann.get('image_id')\n",
        "        if img_id not in images:\n",
        "            skipped_boxes += 1\n",
        "            continue\n",
        "\n",
        "        img_info = images[img_id]\n",
        "        w, h = img_info['width'], img_info['height']\n",
        "        if w <= 0 or h <= 0:\n",
        "            skipped_boxes += 1\n",
        "            continue\n",
        "\n",
        "        x_min, y_min, box_w, box_h = ann['bbox']\n",
        "        if box_w <= 0 or box_h <= 0:\n",
        "            skipped_boxes += 1\n",
        "            continue\n",
        "\n",
        "        x_c = (x_min + box_w / 2) / w\n",
        "        y_c = (y_min + box_h / 2) / h\n",
        "        wn = box_w / w\n",
        "        hn = box_h / h\n",
        "\n",
        "        txt_name = Path(img_info['file_name']).stem + '.txt'\n",
        "        txt_path = output_label_dir / txt_name\n",
        "        with open(txt_path, 'a') as out:\n",
        "            out.write(f\"{category_mapping[cat_id]} {x_c:.6f} {y_c:.6f} {wn:.6f} {hn:.6f}\\n\")\n",
        "        written_boxes += 1\n",
        "\n",
        "    print(f'Finished {json_path.name}: written={written_boxes}, skipped={skipped_boxes}')\n",
        "    return written_boxes, skipped_boxes\n",
        "\n",
        "\n",
        "def symlink_images(src_dir: Path, dst_dir: Path) -> Tuple[int, int]:\n",
        "    # Symlink images from src_dir into dst_dir. Returns (linked, skipped).\n",
        "    if not src_dir.exists():\n",
        "        raise FileNotFoundError(f'Source image directory not found: {src_dir}')\n",
        "\n",
        "    ensure_dir(dst_dir)\n",
        "    linked, skipped = 0, 0\n",
        "\n",
        "    for img_name in tqdm(os.listdir(src_dir), desc=f'Linking {src_dir.name}'):\n",
        "        src = src_dir / img_name\n",
        "        dst = dst_dir / img_name\n",
        "        if dst.exists() or dst.is_symlink():\n",
        "            skipped += 1\n",
        "            continue\n",
        "        os.symlink(src, dst)\n",
        "        linked += 1\n",
        "\n",
        "    print(f'Linked from {src_dir.name}: linked={linked}, skipped_existing={skipped}')\n",
        "    return linked, skipped\n",
        "\n",
        "\n",
        "def write_data_yaml(base_dataset_dir: Path, yaml_path: Path, names: List[str]) -> None:\n",
        "    data_cfg = {\n",
        "        'path': str(base_dataset_dir),\n",
        "        'train': 'images/train',\n",
        "        'val': 'images/val',\n",
        "        'nc': len(names),\n",
        "        'names': names,\n",
        "    }\n",
        "    ensure_dir(yaml_path.parent)\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.safe_dump(data_cfg, f, sort_keys=False)\n",
        "    print(f'Wrote data config: {yaml_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Dataset download and preparation\n",
        "# -----------------------------\n",
        "\n",
        "dataset_path = Path(kagglehub.dataset_download(CFG.kaggle_dataset))\n",
        "print(f'Dataset root: {dataset_path}')\n",
        "\n",
        "coco_root = dataset_path / 'coco2017'\n",
        "annotations_dir = coco_root / 'annotations'\n",
        "train_json = annotations_dir / 'instances_train2017.json'\n",
        "val_json = annotations_dir / 'instances_val2017.json'\n",
        "train_images_src = coco_root / 'train2017'\n",
        "val_images_src = coco_root / 'val2017'\n",
        "\n",
        "for p in [coco_root, annotations_dir, train_json, val_json, train_images_src, val_images_src]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f'Missing expected COCO path: {p}')\n",
        "\n",
        "dirs = prepare_dirs(CFG.work_dir)\n",
        "print(dirs)\n",
        "\n",
        "convert_coco_json_to_yolo(train_json, dirs['train_labels'], CLASS_MAPPING, clear_existing=True)\n",
        "convert_coco_json_to_yolo(val_json, dirs['val_labels'], CLASS_MAPPING, clear_existing=True)\n",
        "\n",
        "symlink_images(train_images_src, dirs['train_images'])\n",
        "symlink_images(val_images_src, dirs['val_images'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write YOLO data.yaml\n",
        "yaml_path = CFG.data_yaml_dir / 'data.yaml'\n",
        "write_data_yaml(CFG.work_dir / 'final', yaml_path, CLASS_NAMES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Training / validation\n",
        "# -----------------------------\n",
        "\n",
        "RUN_TRAINING = False  # Set True to train/resume\n",
        "RESUME_TRAINING = True\n",
        "\n",
        "# Use your finetuned checkpoint if available; otherwise start from a small fast model.\n",
        "weights_path = CFG.model_weights if Path(CFG.model_weights).exists() else CFG.fallback_model\n",
        "model = YOLO(weights_path)\n",
        "print(f'Using weights: {weights_path}')\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    model.train(\n",
        "        data=str(yaml_path),\n",
        "        epochs=CFG.epochs,\n",
        "        imgsz=CFG.imgsz,\n",
        "        batch=CFG.batch,\n",
        "        workers=CFG.workers,\n",
        "        device=CFG.device,\n",
        "        project=CFG.train_project,\n",
        "        name=CFG.train_name,\n",
        "        save_period=1,\n",
        "        exist_ok=True,\n",
        "        resume=RESUME_TRAINING,\n",
        "        # Free-tier survival settings\n",
        "        time=CFG.time_hours,              # hard stop by time budget\n",
        "        fraction=CFG.fraction,            # use subset first, then scale up\n",
        "        freeze=CFG.freeze_backbone_layers,\n",
        "        cache=False,\n",
        "        amp=True,\n",
        "    )\n",
        "\n",
        "metrics = model.val(data=str(yaml_path))\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to train within 1-hour free session limits\n",
        "\n",
        "1. Keep `RUN_TRAINING=True`, `RESUME_TRAINING=True`, and keep `save_period=1` so every epoch checkpoint is saved.\n",
        "2. Keep `time_hours=0.8` to stop safely before forced disconnect.\n",
        "3. Start with `fraction=0.25` and `imgsz=512` (or 416) to iterate fast.\n",
        "4. Use small model fallback (`yolo11n.pt`) until pipeline stabilizes; then switch back to your custom checkpoint.\n",
        "5. Train incrementally across sessions: each new session resumes from the latest checkpoint.\n",
        "6. After convergence trend is good, increase `fraction` to `0.5` then `1.0` for final runs.\n",
        "\n",
        "This approach converts one impossible 50-hour run into many resumable short runs.\n"
        "\n",
        "model = YOLO(CFG.model_weights)\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    model.train(\n",
        "        data=str(yaml_path),\n",
        "        epochs=CFG.epochs,\n",
        "        imgsz=CFG.imgsz,\n",
        "        batch=CFG.batch,\n",
        "        workers=CFG.workers,\n",
        "        device=CFG.device,\n",
        "        project=CFG.train_project,\n",
        "        name=CFG.train_name,\n",
        "        save_period=1,\n",
        "        exist_ok=True,\n",
        "        resume=RESUME_TRAINING,\n",
        "    )\n",
        "\n",
        "metrics = model.val(data=str(yaml_path))\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "coco2017_2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
